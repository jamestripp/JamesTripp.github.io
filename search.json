[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James Tripp",
    "section": "",
    "text": "I’ve worked in research and technology for ten years - from a researcher in psychology to an academic technologist in an interdisciplinary centre and now as a senior research software engineer in central services at the University of Warwick.\nYou can view details of my public projects here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "James Tripp",
    "section": "",
    "text": "R and Docker\n\n\n\n\n\n\n\nPresentation\n\n\n \n\n\n\n\nSep 15, 2022\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nQuarto: a library to run them all?\n\n\n\n\n\n\n\nPresentation\n\n\n \n\n\n\n\nSep 7, 2022\n\n\nCarlos Cámara-Menoyo, Cagatay Turkay, James Tripp\n\n\n\n\n\n\n\n\nGrapho (Alpha)\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMay 15, 2022\n\n\nJames Tripp, Gregory McInerny\n\n\n\n\n\n\n\n\nIntroductory Social Media Analysis\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nFeb 1, 2022\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nSQL Masterclass\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nNov 25, 2021\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nBackfillz\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMay 4, 2020\n\n\nJames Tripp, Gregory McInerny\n\n\n\n\n\n\n\n\nLE-CAT: Lexicon-based Categorization and Analysis Tool\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMar 20, 2020\n\n\nJames Tripp, Noortje Marres\n\n\n\n\n\n\n\n\nQSTEP: Statistical workshop\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nAug 18, 2019\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nMasterclass: Social Media\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nMar 18, 2019\n\n\nJames Tripp\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/lecat.html",
    "href": "projects/lecat.html",
    "title": "LE-CAT: Lexicon-based Categorization and Analysis Tool",
    "section": "",
    "text": "I developed LE-CAT for Professor Noortje Marres. The software is written in R and searches for words in a text. Each word is associated with a category. The presence of the word (e.g., Tony Blair) is assumed to indicate the presence of the category in the text (e.g., politics). Summary information about category co-occurence is displayed displayed to the user as is a network of category and word co-occurence.\nAn interesting feature of the software is that it is an R package containing a shiny app. I could teach with the tool to users who (a) were comfortable with R, (b) preferred a GUI interface running within R (the shiny app), and (c) much preferred only a web browser interface. At CIM I ran LE-CAT within docker containers via the shinyproxy application so that each user could log into one of our CIM servers and use LE-CAT without R installed.\nThe software was used for both research and in teaching environments.\n\nProject Description\nLE-CAT is a Lexicon-based Categorization and Analysis Tool developed by the Centre for Interdisciplinary Methodologies in collaboration with the Media of Cooperation Group at the University of Siegen.\nThe tool allows you to apply a set of word queries associated with a category (a lexicon) to a data set of textual sources (the corpus). LE-CAT determines the frequency of occurrence for each query and category in the corpus, as well as the relations between categories (co-occurrence) by source.\nThe purpose of this technique is to automate and scale up user-led data analysis as it allows the application of a custom-built Lexicon to large data sets. The quick iteration of analysis allows the user to refine a corpus and deeply analyse a given phenomenon.\nLE-CAT was coded by James Tripp. It has been used to support the workshops Youtube as Test Society (University of Siegen), Parking on Twitter (University of Warwick) and the Digital Test of the News (University of Warwick) and is part of the CIM module Digital Object, Digital Methods.\nAcademic correspondence should be sent to Noortje Marres."
  },
  {
    "objectID": "projects/qstep-statistical-workshops.html",
    "href": "projects/qstep-statistical-workshops.html",
    "title": "QSTEP: Statistical workshop",
    "section": "",
    "text": "QSTEP asked me to support their students in their statistical methods modules. The aim was to provide hands on R work with various statistical methods, as used in my Psychology PhD, including:\n\nBivariate regression\nLinear Regression\nMultiple Regression\nMultinomial Logit\nProportional Odds\n\nA focus throughout the sessions were to consider why one model is chosen, what research question it addresses, and how to evaluate the models ability to capture (or failure to capture) the pattern in the data.\nLinks to the rendered output are below. For the original R markdown files see the link above.\n\n\n\nFile\nTopic and HTML link\n\n\n\n\nregression-1-1.Rmd\nBasic introduction to bivariate regression\n\n\nregression-1-2.Rmd\nSimple model evaluation\n\n\nregression-1-3.Rmd\nApplied example using the British Attitudes Survey data\n\n\nregression-2-1.Rmd\nBasic introduction to linear regression\n\n\nregression-2-2.Rmd\nBasic multiple regression\n\n\nregression-2-3.Rmd\nApplied example using the British Attitudes Survey data\n\n\nbridge.Rmd\nSummary/catch-up workshop\n\n\nbridge2.html\nWorking with categorical data"
  },
  {
    "objectID": "projects/qstep-social-media.html",
    "href": "projects/qstep-social-media.html",
    "title": "Masterclass: Social Media",
    "section": "",
    "text": "Code on GitHub\nRStudio:\nI was asked to deliver a Social Media Data Analysis workshop in R by the QSTEP centre at the University of Warwick. I decided to write the Masterclass entirely, where possible, using the tidyverse packages. It was a delightful learning experience and I would suggest anyone face the challenge of putting together such a workshop.\nThe workshop covered methods for downloading, analysing and visualising social media data using the R programming language. We use the ‘tidyverse’ in R and (optionally) the spacy python module for natural language processing."
  },
  {
    "objectID": "projects/qstep-social-media.html#outline",
    "href": "projects/qstep-social-media.html#outline",
    "title": "Masterclass: Social Media",
    "section": "Outline",
    "text": "Outline\nThe structure of the workshop is as follows\n\n\n\nStage\nTitle\nDetail\nR package(s)\n\n\n\n\n\nIntroduction\nOverview of the day\n\n\n\n\nR intro\nAn introduction to R\nggplot2, tidyverse\n\n\nCollection\nScraping\nDownloading and filtering html pages\nrvest, tidyverse, magittr, ggplot2, tibble\n\n\n\nAPI and data dumps\nAccessing data directly using APIs\nhttr, jsonlite, dplyr, textclean, stringr, ggplot2, tidyverse, magittr, tibble, twitteR, RedditExtractoR\n\n\nAnalysis\nSummarising\nTidyverse enabled summaries of our collected data\ntidyverse, tidytext, dplyr, tidyr\n\n\n\nText analysis\nApplying numerical analysis to our text\ntidytext, tidyverse, dplyr, stringr, RedditExtractoR, tidyr, igraph, ggraph, wordcloud, reshape2, tm, topicmodels\n\n\n\nNatural Language\nOptional section using the cleanNLP package\ncleanNLP, tibble, tidyverse, RedditExtractoR, reticulate"
  },
  {
    "objectID": "projects/qstep-sql.html",
    "href": "projects/qstep-sql.html",
    "title": "SQL Masterclass",
    "section": "",
    "text": "Designing and delivering this workshop was a lot of fun. The goal was to introduce Structured Query Language. Students were provided with a containerised Linux machine so they could work with the SQL in a live environment (the Ansible playbook used can be seen here).\nTwo datasets were imported in the database. The first was a rather nice geospatial dataset containing the world borders. The second were world development indicators available via the world bank - superb datasource! Full details are here.\nStudents then (a) carried out basic SQL queries, (b) aggregated and joined data, (c) imported the data from the database into R via an SSH tunnel and RPostgreSQL to create choropleths, and (d) explored additional datasets.\nFeedback from students was very good with this one. I may even have encouraged some aspiring data scientists :)."
  },
  {
    "objectID": "projects/backfillz.html",
    "href": "projects/backfillz.html",
    "title": "Backfillz",
    "section": "",
    "text": "This software provides several novel visualisations for exploring MCMC chains. I worked with MCMC chains during the my post-doc in Psychology when estimating the posterior distributions of cognitive models given the data and found this project very interesting. The software has been released and is considered complete. Fun stuff.\n\nProject Description\nBackfillz.R provides new visual diagnostics for understanding MCMC (Markov Chain Monte Carlo) analyses and outputs. MCMC chains can defy a simple line graph. Unless the chain is very short (which isn’t often the case), plotting tens or hundreds of thousands of data points reveals very little other than a ‘trace plot’ where we only see the outermost points. Common plotting methods may only reveal when an MCMC really hasn’t worked, but not when it has. BackFillz.R slices and dices MCMC chains so increasingly parameter rich, complex analyses can be visualised meaningfully. What does ‘good mixing’ look like? Is a ‘hair caterpillar’ test verifiable? What does a density plot show and what does it hide?"
  },
  {
    "objectID": "projects/quarto-rseconf.html",
    "href": "projects/quarto-rseconf.html",
    "title": "Quarto: a library to run them all?",
    "section": "",
    "text": "Slides\n\n\n\n\nAbstract\nUsing literate programming is a widespread practice amongst data scientists. This practice not only encourages data scientists to produce transparent, rich and reflective accounts of their analysis without the extra overhead of switching between tools, but also leads to artefacts (i.e., notebooks) that are increasingly becoming a medium for dissemination, reproducibility and education. Rmarkdown or Jupyter notebooks, are two of the most well known and used options. While both solutions can be used with multiple programming languages, the decision of whether to use one or the other is almost certain to be exclusively based on that. At least until now, with Quarto being mature enough to become a game-changer.\nQuarto is a language-agnostic software based on Pandoc to render files combining markdown and code into multiple ranges of formats and outputs. As a result, it can be used with either R, Python or Julia without any other dependencies.\nThis collaborative workshop will be structured as follows:\n\na brief theoretical introduction and instructions;\na task that participants may choose from the different use cases provided (i.e. generating a single document, migrating from Rmarkdown or Jupyter, creating a book or generating interactive content); and\na group discussion and conclusions.\n\nParticipants in this workshop have fun while gaining enough depth of breath and practice to evaluate how feasible it is to use Quarto in different scenarios and, ultimately, if it can become the one tool for authoring reproducible scientific or technical documents, regardless of your language of choice."
  },
  {
    "objectID": "projects/introductory-social-media-analysis.html",
    "href": "projects/introductory-social-media-analysis.html",
    "title": "Introductory Social Media Analysis",
    "section": "",
    "text": "Rudimentary example of collecting data from Twitter aimed at introducing R and data collection to humanities students."
  },
  {
    "objectID": "projects/grapho.html",
    "href": "projects/grapho.html",
    "title": "Grapho (Alpha)",
    "section": "",
    "text": "I wrote this software alongside Greg, a fantastic visualisation academic, and it remains unfinished. The software uses a callback function to record commands passed to the R interpreter into a text file and records the plots created. All of these are then saved with date annotation.\nI moved to CIM part way through a rewrite of the package loading process. If anyone would like to fork or futher develop the software then please do get in touch.\n\nProject Description\nGrapho is an R package for recording the commands and visualisations created in an R session. Functions are provided for parsing and visualising the user workflow.\nGrapho is part of WAYS: What aren’t you seeing. The goal of this Turing funded research project is to ‘develop tools which enhance people’s capacity to visualise data, by letting them see what can and can’t be seen in the visualisation.’.\nThe version in this repository is currently under active development. A full release is forthcoming. Please see the roadmap for more details.\nThe package is written by Dr James Tripp and Dr Greg McInerny. Please contact James with any technical issues."
  },
  {
    "objectID": "projects/RandDocker.html",
    "href": "projects/RandDocker.html",
    "title": "R and Docker",
    "section": "",
    "text": "Slides\n\n\n\n\nAbstract\nDocker is a container technology which allows you to specify and build a compute environment. You can run the environment on your local machine and both the instructions for creating the environment and the environment itself can be shared. In this talk I will (a) introduce containers, (b) discuss a worked example of using Docker with R, and (c) consider how containers can help us do better research. Hopefully this presentation will help those of you who have never used Docker to consider if you want to use Docker in your own work and where to start."
  }
]