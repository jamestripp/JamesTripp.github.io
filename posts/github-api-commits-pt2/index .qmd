---
title: "Downloading Github Commits pt 2"
author: "<a href='https://orcid.org/0000-0003-2471-3411'>James Tripp <img alt='ORCID logo' src='https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png' width='16' height='16' /></a>"
date: "2022-05-28"
categories: [visualisation, code, api]
---

Visualising commits over time.

In [part 1](posts/github-api-commits/) we downloaded and parsed the most recent github commits for the [Omeka S repository](https://github.com/omeka/omeka-s).

One page is nice, but more pages is nicer. The URL structure for API requests is straight forward.

https://api.github.com/repos/{user/organisation}/{repo}/commits?pages={page_number}

Defining a function taking the page number, organisation and repo is straight forward.

```{python}
def download_json(url: str) -> list:
    resp = requests.get(url)
    json_data = json.loads(resp.text)
    return(json_data)

def get_commit_api_response(organisation: str, repo: str, page: int) -> list:
    base_url = "https://api.github.com/repos"
    query_url = f"{base_url}/{organisation}/{repo}/commits?pages={int(page)}"
    print(query_url)
    json_data = download_json(query_url)
    return(json_data)
```

Next we want a function to filter one json entry. We are also going to download the total size of the commit from the possibly truncated blog in the commit tree.

```{python}
from pprint import pprint
def filter_commit(commit: dict) -> dict:
    result = commit["commit"]["author"]
    tree_url = commit["commit"]["tree"]["url"]
    tree_data = download_json(tree_url)["tree"]
    pprint(tree_data)
    size = 0
    for branch in tree_data:
        if "blob" in branch:
            size = size + branch["size"]

    result["size"] = size

    return(result)

```

Let us try this out on 1 page.

```{python}
import requests
import json
data = get_commit_api_response("omeka", "omeka-s", 1)
this_commit = data[0]
filter_commit(this_commit)
```

